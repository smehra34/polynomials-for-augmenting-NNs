comment: '7/2022: Used for the ECCV --- compressed version (4.3 mil params).'
learning_rate: 0.1

exp_name: test_label_smoothing

dataset:
  root: /tmp/
  batch_size: 128
  db: cifar

model:
  fn: pdc_nosharing.py
  name: PDC_wrapper
  args:
    train: True
    use_alpha: True
    n_lconvs: 1
    use_lactiv: True
    norm_local: 1
    num_blocks: [2, 2, 2, 1]
    kern_loc: 3
    kern_loc_so: 3
    norm_x: 0
    what_lactiv: 2
    use_uactiv: True
    n_channels: [64, 128, 192, 256]
    planes_ho:  [64, 64, 128, 128]
    use_only_first_conv: True
    train_time_activ: regularised # None, 'fixed_increment', or 'regularised'
    # pretrained_params_path: results_poly/prelu_fixed_04/latest.pth80.tar

training_info:
  total_epochs: 120
  display_interval: 200
  lr_milestones: [40, 60, 80, 100]
  lr_gamma: 0.1
  label_smoothing: 0.1
  train_time_activations:
    start_increment_epoch: 1
    end_increment_epoch: 60
    increment_patience: 5
    param_threshold: 0.99
    epochs_before_regularisation: 0
    init_regularisation_w: 0.01
    scheduler:
      increase_factor: 2
      patience: 10
