comment: '7/2022: Used for the ECCV --- compressed version (4.3 mil params).'
learning_rate: 0.1

exp_name: 60_epoch_tta

dataset:
  root: /tmp/
  batch_size: 128
  db: cifar

model:
  fn: pdc_nosharing.py
  name: PDC_wrapper
  args:
    train: True
    use_alpha: True
    n_lconvs: 1
    use_lactiv: True
    norm_local: 1
    num_blocks: [2, 2, 2, 1]
    kern_loc: 3
    kern_loc_so: 3
    norm_x: 0
    what_lactiv: 2
    use_uactiv: True
    n_channels: [64, 128, 192, 256]
    planes_ho:  [64, 64, 128, 128]
    use_only_first_conv: True
    train_time_activ: True
    activ_param_threshold: 0.99

training_info:
  total_epochs: 120
  epochs_before_activ_regularisation: 0
  display_interval: 200
  lr_milestones: [40, 60, 80, 100]
  lr_gamma: 0.1
  regularisation_w: 0.003
